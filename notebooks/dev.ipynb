{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from ..src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08915b2120042c78e85ce856ff59b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qwen_path = \"D:\\models\\Qwen2.5-7B\" # change it\n",
    "\n",
    "qwen = AutoModel.from_pretrained(qwen_path, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(qwen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = convert_to_Qwen2_ND(qwen).to(\"cuda\")\n",
    "qwen.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen = impacts_off(qwen) # comment this to test impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "d:\\Desktop\\9сем\\CreationOfIntelligentSystems_Multilingual_Interpretability\\modules.py:183: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = qwen(tokenizer([\n",
    "        \"Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123\", \n",
    "        \"Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456\",\n",
    "    ], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = detect_domain_specific_neurons(\n",
    "    qwen,\n",
    "    dataloader=None,\n",
    "    eps=1e-2,\n",
    "    domain_name=\"eng\", \n",
    "    reset_impacts=False,\n",
    "    reset_dsn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3613, 0.4199, 0.2500,  ..., 0.0022, 0.2559, 0.6680],\n",
       "         [0.3496, 0.3691, 0.2617,  ..., 0.0018, 0.2070, 0.6953]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " {'eng': {'up_proj': tensor([ True,  True,  True,  ..., False,  True,  True], device='cuda:0'),\n",
       "   'down_proj': tensor([ True,  True,  True,  ..., False,  True,  True], device='cuda:0')}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.layers[0].mlp.up_proj.impacts, qwen.layers[0].mlp.dsn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
