{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader,IterableDataset\n",
    "from datasets import load_dataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_path = \"D:\\models\\Qwen2.5-0.5B\" # change it\n",
    "\n",
    "qwen = Qwen2ForCausalLM.from_pretrained(qwen_path, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(qwen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = convert_to_Qwen2_ND(qwen).to(\"cuda\")\n",
    "qwen.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen = impacts_off(qwen) # comment this to test impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "d:\\Desktop\\9сем\\CreationOfIntelligentSystems_Multilingual_Interpretability\\src\\modules.py:183: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = qwen(tokenizer([\n",
    "        \"Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123\", \n",
    "        \"Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456\",\n",
    "    ], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen, _ = detect_domain_specific_neurons(\n",
    "    qwen,\n",
    "    tokenizer,\n",
    "    dataloader=None,\n",
    "    eps=1e-2,\n",
    "    domain_name=\"eng\", \n",
    "    reset_impacts=False,\n",
    "    reset_dsn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model=None,\n",
    "        train_config=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_config = train_config if train_config is not None else {}\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader=None,\n",
    "        val_loader=None,\n",
    "        model=None,\n",
    "        only_domain_specific=True,\n",
    "        domain=None,\n",
    "        max_iters=None,\n",
    "        optimizer=None,\n",
    "        lr=None,\n",
    "        ignore_index=None,\n",
    "        evaluate_every=-1,  # -1 for no evaluation\n",
    "    ):\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model provided\")\n",
    "        \n",
    "        if lr is not None:\n",
    "            self.train_config[\"lr\"] = lr\n",
    "        elif \"lr\" in self.train_config:\n",
    "            lr = self.train_config[\"lr\"]\n",
    "        else:\n",
    "            warnings.warn(\"No learning rate provided. Defaulting to 1e-3\", UserWarning)\n",
    "            lr = 1e-3\n",
    "        \n",
    "        if max_iters is not None:\n",
    "            self.train_config[\"max_iters\"] = max_iters\n",
    "        elif \"max_iters\" in self.train_config:\n",
    "            max_iters = self.train_config[\"max_iters\"]\n",
    "        else:\n",
    "            max_iters = 1\n",
    "            self.train_config[\"max_iters\"] = max_iters\n",
    "            warnings.warn(\"No max_iters provided. Defaulting to 100\", UserWarning)\n",
    "\n",
    "        if ignore_index is None:\n",
    "            ignore_index = self.train_config.get(\"ignore_index\", None)\n",
    "        else:\n",
    "            self.train_config[\"ignore_index\"] = ignore_index\n",
    "        \n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction=\"sum\")\n",
    "        \n",
    "        if evaluate_every is not None:\n",
    "            self.train_config[\"evaluate_every\"] = evaluate_every\n",
    "        elif \"evaluate_every\" in self.train_config:\n",
    "            evaluate_every = self.train_config[\"evaluate_every\"]\n",
    "        else:\n",
    "            warnings.warn(\"No evaluate_every provided. Defaulting to -1 (no evaluation)\", UserWarning)\n",
    "            evaluate_every = -1\n",
    "        \n",
    "        if domain is not None:\n",
    "            self.train_config[\"domain\"] = domain\n",
    "        elif \"domain\" in self.train_config:\n",
    "            domain = self.train_config[\"domain\"]\n",
    "        else:\n",
    "            warnings.warn(\"No domain provided. Defaulting to 'eng'\", UserWarning)\n",
    "            domain = \"eng\"\n",
    "        \n",
    "        # IMPORTANT!\n",
    "        if only_domain_specific:\n",
    "            # calculate gradients only for layers with possible occurence of DSN\n",
    "            dsn_model_grads_to_train(self.model)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            if isinstance(optimizer, torch.optim.Optimizer):\n",
    "                optimizer = optimizer([param for param in self.model.parameters() if param.requires_grad], lr=lr)\n",
    "            else:\n",
    "                self.train_config[\"optimizer\"] = optimizer\n",
    "        elif \"optimizer\" in self.train_config:\n",
    "            opt_name = self.train_config[\"optimizer\"]\n",
    "        else:\n",
    "            warnings.warn(\"No optimizer provided. Defaulting to Adam\", UserWarning)\n",
    "            opt_name = \"Adam\"\n",
    "\n",
    "        if opt_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam([param for param in self.model.parameters() if param.requires_grad], lr=lr)\n",
    "        elif opt_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD([param for param in self.model.parameters() if param.requires_grad], lr=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {opt_name}\")\n",
    "        \n",
    "        losses = []\n",
    "        metrics = []\n",
    "        n_iter = 0\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        with tqdm(range(max_iters), desc=\"Training iters\") as pbar:\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output = self._forward(batch, device=device)[\"logits\"]\n",
    "                loss = loss_function(output.view(-1, output.shape[-1]), batch[0].to(device).view(-1))\n",
    "                loss.backward()\n",
    "                # IMPORTANT!\n",
    "                if only_domain_specific:\n",
    "                    dsn_model_mask_gradients(self.model, domain=domain)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                n_iter += len(batch)\n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                if evaluate_every > 0 and n_iter >= (len(metrics) + 1) * evaluate_every:\n",
    "                    eval_metrics = self.evaluate(val_loader)\n",
    "                    metrics.append(eval_metrics)\n",
    "                    self.model.train()\n",
    "            \n",
    "                if n_iter >= max_iters:\n",
    "                    break\n",
    "\n",
    "        self._metrics = metrics\n",
    "        self._losses = losses\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def _forward(\n",
    "        self, \n",
    "        batch, \n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        input_ids, attention_mask = batch[0].to(device), batch[1].to(device) \n",
    "        preds = self.model(input_ids, attention_mask)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def _calulate_metrics(\n",
    "        self, \n",
    "        output, \n",
    "        batch, \n",
    "        device=\"cuda\"\n",
    "    ):  \n",
    "        metrics_to_calulate = self.train_config.get(\"eval_metrics\", [])\n",
    "        metrics = {}\n",
    "\n",
    "        for m in metrics_to_calulate:\n",
    "            if m == \"loss\" or m == \"cross_entropy_loss\":\n",
    "                loss_function = nn.CrossEntropyLoss(ignore_index=self.train_config.get(\"ignore_index\", None))\n",
    "                metrics[m] = loss_function(output.view(-1, output.shape[-1]), batch[0].to(device).view(-1)).detach().cpu().numpy()\n",
    "            else:\n",
    "                warnings.warn(f\"Metric: {m} NOT IMPLEMENTED\", UserWarning)\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        val_loader=None,\n",
    "    ):\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                device = next(self.model.parameters()).device\n",
    "\n",
    "                metrics = []\n",
    "                # for batch in tqdm(val_loader, leave=False, desc=\"eval batch\"):\n",
    "                for batch in val_loader:\n",
    "                    self.model.train()\n",
    "\n",
    "                    output = self._forward(batch, device=device)[\"logits\"]\n",
    "                    \n",
    "                    batch_metrics = self._calulate_metrics(output, batch, device)\n",
    "\n",
    "                    metrics.append(batch_metrics)\n",
    "\n",
    "                metrics_mean = {}\n",
    "                for metric in metrics[0].keys():\n",
    "                    metrics_mean[metric] = np.mean([m[metric] for m in metrics])\n",
    "            \n",
    "                return metrics_mean\n",
    "\n",
    "        else:\n",
    "            warnings.warn(\"No validation data provided, return empty val metrics\", UserWarning)\n",
    "            return {}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer)\n",
    "dataset = load_dataset('yhavinga/ccmatrix', \"en-ru\", split='train', streaming=True, trust_remote_code=True)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn = collator,\n",
    "    batch_sampler=None \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=896, out_features=896, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8913d38bd691418da9d7984bce3272bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iters:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=qwen, \n",
    "    train_config={\n",
    "        \"eval_metrics\": [\"loss\"],\n",
    "        \"lr\": 1e-4,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"max_iters\": 500, \n",
    "        \"ignore_index\": tokenizer.pad_token_id\n",
    "    }\n",
    ")\n",
    "\n",
    "qwen = trainer.train(\n",
    "    train_loader=data_loader,\n",
    "    val_loader=None,\n",
    "    evaluate_every=-1,\n",
    "    only_domain_specific=True,\n",
    "    domain=\"eng\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
