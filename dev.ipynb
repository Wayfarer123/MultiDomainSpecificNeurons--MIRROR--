{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.qwen2.modeling_qwen2 import Qwen2SdpaAttention, Qwen2MLP, Qwen2RMSNorm, Qwen2RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_path = \"D:\\models\\Qwen2.5-3B\" # change it\n",
    "\n",
    "qwen = AutoModel.from_pretrained(qwen_path, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(qwen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class Qwen2MLP_ND(nn.Module):\n",
    "    def __init__(self, mlp=None):\n",
    "        super().__init__()\n",
    "        self.__from_MLP(mlp)\n",
    "        self.up_proj.impacts = None\n",
    "        self.down_proj.impacts = None\n",
    "        self.calculate_impacts = True\n",
    "\n",
    "    def __from_MLP(self, mlp):\n",
    "        self.hidden_size = mlp.hidden_size\n",
    "        self.intermediate_size = mlp.intermediate_size\n",
    "        self.gate_proj = deepcopy(mlp.gate_proj)\n",
    "        self.up_proj = deepcopy(mlp.up_proj)\n",
    "        self.down_proj = deepcopy(mlp.down_proj)\n",
    "        self.act_fn = deepcopy(mlp.act_fn)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        '''Implements the MLP forward and Parallell Neuron Detection\n",
    "        1. Applies the MLP to the hidden state\n",
    "        2. Calculates Neurons Impacts = ||(act_fn(W_gate * hidden_state) * W_up * mask) * W_down||_2, \n",
    "        where\n",
    "            mask = np.eye(hidden_state.shape[0])\n",
    "            W_down = self.down_proj\n",
    "            W_up = self.up_proj\n",
    "            W_gate = self.gate_proj\n",
    "            act_fn = self.act_fn\n",
    "        return both new hidden state and Neurons Impacts\n",
    "        '''\n",
    "        intermediate_state = self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state)\n",
    "\n",
    "        if self.calculate_impacts:\n",
    "            with torch.no_grad():                \n",
    "                # impacts = torch.norm(self.down_proj(\n",
    "                #     intermediate_state.unsqueeze(-1) * \\\n",
    "                #     torch.eye(intermediate_state.shape[-1], dtype=intermediate_state.dtype).unsqueeze(0).unsqueeze(0).to(intermediate_state.device)\n",
    "                # ), dim=[1,-1]).detach()\n",
    "                \n",
    "                impacts = torch.sum((intermediate_state ** 2).sum(dim=1).unsqueeze(-1) * (self.up_proj.weight.unsqueeze(0) ** 2), dim=-1) ** 0.5\n",
    "\n",
    "                # impacts = torch.norm(intermediate_state.unsqueeze(-1) * self.up_proj.weight.unsqueeze(0).unsqueeze(0), dim=[1,-1]).detach()\n",
    "\n",
    "                # slow option\n",
    "                # impacts = torch.zeros([intermediate_state.shape[0], intermediate_state.shape[-1]])\n",
    "                # for i in range(intermediate_state.shape[-1]):\n",
    "                #     mask = torch.zeros([intermediate_state.shape[-1]], dtype=intermediate_state.dtype).to(intermediate_state.device)\n",
    "                #     mask[i] = 1\n",
    "                #     impacts[:,i] = torch.norm(self.down_proj(intermediate_state * mask), dim=[1,-1])\n",
    "                    \n",
    "                if self.up_proj.impacts is None:\n",
    "                    self.up_proj.impacts = impacts\n",
    "                else:\n",
    "                    self.up_proj.impacts = torch.cat((self.up_proj.impacts, impacts), dim=0)\n",
    "                if self.down_proj.impacts is None:\n",
    "                    self.down_proj.impacts = impacts\n",
    "                else:\n",
    "                    self.down_proj.impacts = torch.cat((self.down_proj.impacts, impacts), dim=0)\n",
    "                    \n",
    "        return self.down_proj(intermediate_state)\n",
    "    \n",
    "\n",
    "class Qwen2SdpaAttention_ND(nn.Module):\n",
    "    def __init__(self, sdpa=None):\n",
    "        super().__init__()\n",
    "        self.__from_Sdpa(sdpa)\n",
    "        self.q_proj.impacts = None\n",
    "        self.k_proj.impacts = None\n",
    "        self.calculate_impacts = True\n",
    "\n",
    "    def __from_Sdpa(self, sdpa):\n",
    "        self.hidden_size = sdpa.hidden_size\n",
    "        self.num_heads = sdpa.num_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = sdpa.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = sdpa.max_position_embeddings\n",
    "        self.rope_theta = sdpa.rope_theta\n",
    "        self.is_causal = sdpa.is_causal\n",
    "        self.attention_dropout = sdpa.attention_dropout\n",
    "        self.layer_idx = sdpa.layer_idx\n",
    "\n",
    "        self.q_proj = deepcopy(sdpa.q_proj)\n",
    "        self.k_proj = deepcopy(sdpa.k_proj)\n",
    "        self.v_proj = deepcopy(sdpa.v_proj)\n",
    "        self.o_proj = deepcopy(sdpa.o_proj)\n",
    "\n",
    "        self.rotary_emb = deepcopy(sdpa.rotary_emb)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        '''Implements the Sdpa Attention forward and Parallell Neuron Detection\n",
    "        1. Applies the Sdpa Attention to the hidden state\n",
    "        2. Calculates Neurons Impacts = ||softmax((W_q(x) * W_k(x)^T -Delta(x) / sqrt(head_dim)) - softmax((W_q(x) * W_k(x)^T) / sqrt(head_dim))||_2, \n",
    "        where\n",
    "            Delta(x) = W_Q(x).resize(l, 1, d_mid) * W_K(x).resize(1, l, d_mid)\n",
    "        '''\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        causal_mask = attention_mask\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "\n",
    "        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "        if query_states.device.type == \"cuda\" and attention_mask is not None:\n",
    "            query_states = query_states.contiguous()\n",
    "            key_states = key_states.contiguous()\n",
    "            value_states = value_states.contiguous()\n",
    "\n",
    "        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
    "        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal = True if causal_mask is None and q_len > 1 else False\n",
    "\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attn_mask=causal_mask,\n",
    "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        if self.calculate_impacts:\n",
    "            with torch.no_grad():\n",
    "                Delta_x = (query_states.unsqueeze(-2) * key_states.unsqueeze(-3)).detach()\n",
    "\n",
    "                L, S = query_states.size(-2), key_states.size(-2)\n",
    "                scale_factor = 1 / query_states.size(-1) ** 0.5\n",
    "                attn_bias = torch.zeros(L, S, dtype=query_states.dtype).to(query_states.device)\n",
    "                if is_causal:\n",
    "                    assert causal_mask is None\n",
    "                    temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0).to(query_states.device)\n",
    "                    attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "                    attn_bias.to(query_states.dtype)\n",
    "\n",
    "                if causal_mask is not None:\n",
    "                    if causal_mask.dtype == torch.bool:\n",
    "                        attn_bias.masked_fill_(causal_mask.logical_not(), float(\"-inf\"))\n",
    "                    else:\n",
    "                        attn_bias += causal_mask\n",
    "\n",
    "                attn_weight = query_states @ key_states.transpose(-2, -1) \n",
    "\n",
    "                attn_weight_Delta = (attn_weight.unsqueeze(-1) - Delta_x) * scale_factor\n",
    "                attn_weight = attn_weight * scale_factor\n",
    "\n",
    "                attn_weight += attn_bias\n",
    "                attn_weight_Delta += attn_bias.unsqueeze(-1)\n",
    "\n",
    "                attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "                attn_weight_Delta = torch.softmax(attn_weight_Delta, dim=-2)\n",
    "\n",
    "                attn_weight = torch.dropout(attn_weight, self.attention_dropout if self.training else 0.0, train=True)\n",
    "                attn_weight_Delta = torch.dropout(attn_weight_Delta, self.attention_dropout if self.training else 0.0, train=True)\n",
    "                impacts = torch.norm(attn_weight.unsqueeze(-1) - attn_weight_Delta, dim=[-2,-3]).detach()\n",
    "\n",
    "                if self.q_proj.impacts is None:\n",
    "                    self.q_proj.impacts = impacts\n",
    "                else:\n",
    "                    self.q_proj.impacts = torch.cat((self.q_proj.impacts, impacts), dim=0).detach()\n",
    "                if self.k_proj.impacts is None:\n",
    "                    self.k_proj.impacts = impacts\n",
    "                else:\n",
    "                    self.k_proj.impacts = torch.cat((self.k_proj.impacts, impacts), dim=0).detach()\n",
    "\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_Qwen2_ND(\n",
    "    qwen\n",
    "):\n",
    "    qwen.to(\"cpu\")\n",
    "    for i in range(len(qwen.layers)):\n",
    "        qwen.layers[i].mlp = Qwen2MLP_ND(qwen.layers[i].mlp)\n",
    "        qwen.layers[i].self_attn = Qwen2SdpaAttention_ND(qwen.layers[i].self_attn)\n",
    "\n",
    "    return qwen\n",
    "\n",
    "def reset_impacts(\n",
    "    qwen_nd\n",
    "):\n",
    "    for i in range(len(qwen_nd.layers)):\n",
    "        qwen_nd.layers[i].mlp.up_proj.impacts = None\n",
    "        qwen_nd.layers[i].mlp.down_proj.impacts = None\n",
    "\n",
    "        qwen_nd.layers[i].self_attn.q_proj.impacts = None\n",
    "        qwen_nd.layers[i].self_attn.k_proj.impacts = None\n",
    "    return qwen_nd\n",
    "\n",
    "def impacts_off(\n",
    "    qwen_nd\n",
    "):\n",
    "    for i in range(len(qwen_nd.layers)):\n",
    "        qwen_nd.layers[i].mlp.calculate_impacts = False\n",
    "        qwen_nd.layers[i].self_attn.calculate_impacts = False\n",
    "    return qwen_nd\n",
    "\n",
    "def impacts_on(\n",
    "    qwen_nd\n",
    "):\n",
    "    for i in range(len(qwen_nd.layers)):\n",
    "        qwen_nd.layers[i].mlp.calculate_impacts = True\n",
    "        qwen_nd.layers[i].self_attn.calculate_impacts = True\n",
    "    return qwen_nd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = convert_to_Qwen2_ND(qwen).to(\"cuda\")\n",
    "qwen.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen = impacts_off(qwen) # comment this to test impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = qwen(tokenizer([\n",
    "        \"Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123, Hello World 123\", \n",
    "        \"Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456, Hello World 456\"\n",
    "    ], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8960]), torch.Size([4, 12, 128]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.layers[0].mlp.up_proj.impacts.shape, qwen.layers[0].self_attn.q_proj.impacts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
