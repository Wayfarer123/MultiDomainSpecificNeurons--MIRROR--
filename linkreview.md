# LinkReview

- Here we have collect info about all the works that may be useful for writing our paper
- Each of the contributors is responsible for their part of the work, as specified in the table

> [!NOTE]
> This review table will be updated, so it is not a final version

| Title | Year | Authors | Paper | Code | Summary |
| :--- | :--- | ---: | :--- | :--- | :--- |
| How do Large Language Models Handle Multilingualism? [@Nikita_Okhotnikov](https://github.com/Wayfarer123) | 2024 | Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing | [arxiv preprint](https://arxiv.org/pdf/2402.18815) | - | Authors define "language specific neurons" that dramatically affect the performance on a single language and finetune these on little training corpus gaining noticeable performance uplift |
| Do Llamas Work in English? On the Latent Language of Multilingual Transformers [@Anastasia Voznyuk](https://github.com/natriistorm) | 2024 | Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West | [arxiv preprint](https://arxiv.org/pdf/2402.10588) | [GitHub](https://github.com/epfl-dlab/llm-latent-language) | Authors claim that models operate in English and only after 15th layer transition to the target language. Entropy, at first high, to the final layers, decreases |
| Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models [@Andrei Semenov](https://github.com/Andron00e) | 2024 | Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Wayne Xin Zhao, Furu Wei, Ji-Rong Wen | [arxiv](https://arxiv.org/abs/2402.16438) | [GitHub](https://github.com/RUCAIBox/Language-Specific-Neurons) | There exists lanugage-specific neurons, responsible for generating putput in particular language. Consequently, we can affect the quality of the multilingual output, by activating and deactivating these neurons |
| On the cross-lingual transferability of monolingual representations [@Alexander Terentyev](https://github.com/lopate) | 2020 | Artetxe, Mikel, Sebastian Ruder, and Dani Yogatama. | [arxiv](https://arxiv.org/abs/1910.11856) | - | A masked language model, trained on one language, is transferred to another by learning a new embedding matrix, freezing all other layers. This approach, without shared vocabulary or joint training, rivals multilingual BERT on cross-lingual tasks, showing monolingual models can generalize across languages. Also, new cross-lingual benchmark is added |
| Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs [@Andrei Semenov](https://github.com/Andron00e) | 2024 |Weixuan Wang, Barry Haddow, Wei Peng, Alexandra Birch | [arxiv](https://arxiv.org/abs/2406.09265) | [GitHub](https://github.com/weixuan-wang123/multilingual-neurons) | Study about how neuron activation is shared across languages by categorizing neurons into four types: **all-shared**, **partial-shared**, **specific**, and **non-activated**. Task type affects linguistic sharing patterns, neuron behavior varies across inputs, and all-shared neurons are crucial for correct responses. Increasing all-shared neurons improves accuracy on multilingual tasks. |
| Knowledge Mechanisms in Large Language Models: A Survey and Perspective [@Andrei Semenov](https://github.com/Andron00e) | 2024 | ... | [arxiv](https://arxiv.org/abs/2407.15017) |  | TODO |
| Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review [@Andrei Semenov](https://github.com/Andron00e) | 2023 | ... | [arxiv](https://arxiv.org/abs/2305.16768) | survey | TODO |
| Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models [@Andrei Semenov](https://github.com/Andron00e) | 2024 | ... | [arxiv](https://arxiv.org/abs/2409.12435) | [GitHub](https://github.com/ChenDelong1999/Linguistic-Similarity) | TODO |
| Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners [@Andrei Semenov](https://github.com/Andron00e) | 2024 | Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang | [arxiv](https://arxiv.org/abs/2405.13816) | [GitHub](https://github.com/Shimao-Zhang/LLM-Multilingual-Learner) | 
Most LLMs show unbalanced performance across languages, but translation-based multilingual alignment is effective. This study explores the spontaneous improvement in multilingual alignment when LLMs are instruction-tuned on question translation data (without annotated answers). This boosts alignment between English and many languages, even those not seen during tuning. |
| Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge Representation Sharing in LLMs [@Nikita_Okhotnikov](https://github.com/Wayfarer123) | 2024 | Maxim Ifergan, Leshem Choshen, Roee Aharoni, Idan Szpektor, Omri Abend | [arxiv preprint](https://www.arxiv.org/pdf/2408.10646) | - | LLM factual knowledge are inconsistent across languages. The methodology to measure knowledge representations sharing across languages proposed. Script similarity -- dominant factor in representation sharing. Multiligual sharing has a potential to increase performance in the best-performing language. |

### [Subtopic]: How distinct neurons work in LLMs? 

| Title | Year | Authors | Paper | Code | Summary |
| :--- | :--- | ---: | :--- | :--- | :--- |
| Neurons in Large Language Models: Dead, N-gram, Positional [@Andrei Semenov](https://github.com/Andron00e) | 2024 | Elena Voita, Javier Ferrando, Christoforos Nalmpantis | [arxiv](https://arxiv.org/abs/2309.04827) | [Blog]([https://github.com/RUCAIBox/Language-Specific-Neurons](https://lena-voita.github.io/posts/neurons_in_llms_dead_ngram_positional.html)) | Many neurons are “dead”, i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. |
| Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons [@Andrei Semenov](https://github.com/Andron00e) | 2024 | Yongqi Leng and Deyi Xiong | [arxiv](https://www.arxiv.org/abs/2407.06488) | - | Experiments identify task-specific neurons linked to specific tasks, offering insights into generalization and catastrophic forgetting in multi-task learning. Overlapping neurons across tasks, especially in certain LLM layers, strongly correlate with generalization performance. |
| Evaluating Neuron Interpretation Methods of NLP Models [@Andrei Semenov](https://github.com/Andron00e) | 2023 | Yimin Fan, Fahim Dalvi, Nadir Durrani, Hassan Sajjad | [arxiv](https://arxiv.org/abs/2301.12608) | [GitHub](https://github.com/fdalvi/neuron-comparative-analysis) | Neurons that are commonly discovered by different interpretation methods are more informative than others, and two novel methods for detecting informative neurons are presented|
